{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "\n",
    "CONFIDENCE_THRESHOLD = 0.2\n",
    "NMS_THRESHOLD = 0.4\n",
    "COLORS = [(0, 255, 255), (255, 255, 0), (0, 255, 0), (255, 0, 0)]\n",
    "\n",
    "class_names = []\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    class_names = [cname.strip() for cname in f.readlines()]\n",
    "    print(class_names)\n",
    "vc = cv2.VideoCapture(r\"fruit_images\\apple_video.mp4\")\n",
    "\n",
    "net = cv2.dnn.readNet(\"yolov4-custom_2000.weights\", \"yolov4-custom.cfg\")\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA_FP16)\n",
    "\n",
    "model = cv2.dnn_DetectionModel(net)\n",
    "model.setInputParams(size=(608, 608), scale=1/255, swapRB=True)\n",
    "\n",
    "while cv2.waitKey(1) < 1:\n",
    "    (grabbed, frame) = vc.read()\n",
    "    if not grabbed:\n",
    "        exit()\n",
    "\n",
    "    start = time.time()\n",
    "    classes, scores, boxes = model.detect(frame, CONFIDENCE_THRESHOLD, NMS_THRESHOLD)\n",
    "    end = time.time()\n",
    "\n",
    "    start_drawing = time.time()\n",
    "    print(classes)\n",
    "    for (classid, score, box) in zip(classes, scores, boxes):\n",
    "        print(classid)\n",
    "        color = COLORS[int(classid) % len(COLORS)]\n",
    "        label = \"%s : %f\" % (class_names[classid], score)\n",
    "        cv2.rectangle(frame, box, color, 2)\n",
    "        cv2.putText(frame, label, (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    end_drawing = time.time()\n",
    "    fps_label = \"FPS: %.2f (excluding drawing time of %.2fms)\" % (1 / (end - start), (end_drawing - start_drawing) * 1000)\n",
    "    cv2.putText(frame, fps_label, (0, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "    cv2.imshow(\"detections\", frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "\n",
    "CONFIDENCE_THRESHOLD = 0.2\n",
    "NMS_THRESHOLD = 0.4\n",
    "COLORS = [(0, 255, 255), (255, 255, 0), (0, 255, 0), (255, 0, 0)]\n",
    "\n",
    "class_names = []\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    class_names = [cname.strip() for cname in f.readlines()]\n",
    "    print(class_names)\n",
    "vc = cv2.VideoCapture(r\"fruit_images\\apple_video.mp4\")\n",
    "\n",
    "net = cv2.dnn.readNet(\"yolov4-custom_2000.weights\", \"yolov4-custom.cfg\")\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA_FP16)\n",
    "\n",
    "model = cv2.dnn_DetectionModel(net)\n",
    "model.setInputParams(size=(608, 608), scale=1/255, swapRB=True)\n",
    "\n",
    "while cv2.waitKey(1) < 1:\n",
    "    (grabbed, frame) = vc.read()\n",
    "    if not grabbed:\n",
    "        exit()\n",
    "\n",
    "    start = time.time()\n",
    "    classes, scores, boxes = model.detect(frame, CONFIDENCE_THRESHOLD, NMS_THRESHOLD)\n",
    "    end = time.time()\n",
    "\n",
    "    start_drawing = time.time()\n",
    "    print(classes)\n",
    "    for (classid, score, box) in zip(classes, scores, boxes):\n",
    "        print(classid)\n",
    "        color = COLORS[int(classid) % len(COLORS)]\n",
    "        label = \"%s : %f\" % (class_names[classid], score)\n",
    "        cv2.rectangle(frame, box, color, 2)\n",
    "        cv2.putText(frame, label, (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    end_drawing = time.time()\n",
    "    fps_label = \"FPS: %.2f (excluding drawing time of %.2fms)\" % (1 / (end - start), (end_drawing - start_drawing) * 1000)\n",
    "    cv2.putText(frame, fps_label, (0, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "    cv2.imshow(\"detections\", frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'orange']\n",
      "[INFO] loading YOLO from disk...\n",
      "done\n",
      "('conv_0', 'bn_0', 'mish_1', 'conv_1', 'bn_1', 'mish_2', 'conv_2', 'bn_2', 'mish_3', 'identity_3', 'conv_4', 'bn_4', 'mish_5', 'conv_5', 'bn_5', 'mish_6', 'conv_6', 'bn_6', 'mish_7', 'shortcut_7', 'conv_8', 'bn_8', 'mish_9', 'concat_9', 'conv_10', 'bn_10', 'mish_11', 'conv_11', 'bn_11', 'mish_12', 'conv_12', 'bn_12', 'mish_13', 'identity_13', 'conv_14', 'bn_14', 'mish_15', 'conv_15', 'bn_15', 'mish_16', 'conv_16', 'bn_16', 'mish_17', 'shortcut_17', 'conv_18', 'bn_18', 'mish_19', 'conv_19', 'bn_19', 'mish_20', 'shortcut_20', 'conv_21', 'bn_21', 'mish_22', 'concat_22', 'conv_23', 'bn_23', 'mish_24', 'conv_24', 'bn_24', 'mish_25', 'conv_25', 'bn_25', 'mish_26', 'identity_26', 'conv_27', 'bn_27', 'mish_28', 'conv_28', 'bn_28', 'mish_29', 'conv_29', 'bn_29', 'mish_30', 'shortcut_30', 'conv_31', 'bn_31', 'mish_32', 'conv_32', 'bn_32', 'mish_33', 'shortcut_33', 'conv_34', 'bn_34', 'mish_35', 'conv_35', 'bn_35', 'mish_36', 'shortcut_36', 'conv_37', 'bn_37', 'mish_38', 'conv_38', 'bn_38', 'mish_39', 'shortcut_39', 'conv_40', 'bn_40', 'mish_41', 'conv_41', 'bn_41', 'mish_42', 'shortcut_42', 'conv_43', 'bn_43', 'mish_44', 'conv_44', 'bn_44', 'mish_45', 'shortcut_45', 'conv_46', 'bn_46', 'mish_47', 'conv_47', 'bn_47', 'mish_48', 'shortcut_48', 'conv_49', 'bn_49', 'mish_50', 'conv_50', 'bn_50', 'mish_51', 'shortcut_51', 'conv_52', 'bn_52', 'mish_53', 'concat_53', 'conv_54', 'bn_54', 'mish_55', 'conv_55', 'bn_55', 'mish_56', 'conv_56', 'bn_56', 'mish_57', 'identity_57', 'conv_58', 'bn_58', 'mish_59', 'conv_59', 'bn_59', 'mish_60', 'conv_60', 'bn_60', 'mish_61', 'shortcut_61', 'conv_62', 'bn_62', 'mish_63', 'conv_63', 'bn_63', 'mish_64', 'shortcut_64', 'conv_65', 'bn_65', 'mish_66', 'conv_66', 'bn_66', 'mish_67', 'shortcut_67', 'conv_68', 'bn_68', 'mish_69', 'conv_69', 'bn_69', 'mish_70', 'shortcut_70', 'conv_71', 'bn_71', 'mish_72', 'conv_72', 'bn_72', 'mish_73', 'shortcut_73', 'conv_74', 'bn_74', 'mish_75', 'conv_75', 'bn_75', 'mish_76', 'shortcut_76', 'conv_77', 'bn_77', 'mish_78', 'conv_78', 'bn_78', 'mish_79', 'shortcut_79', 'conv_80', 'bn_80', 'mish_81', 'conv_81', 'bn_81', 'mish_82', 'shortcut_82', 'conv_83', 'bn_83', 'mish_84', 'concat_84', 'conv_85', 'bn_85', 'mish_86', 'conv_86', 'bn_86', 'mish_87', 'conv_87', 'bn_87', 'mish_88', 'identity_88', 'conv_89', 'bn_89', 'mish_90', 'conv_90', 'bn_90', 'mish_91', 'conv_91', 'bn_91', 'mish_92', 'shortcut_92', 'conv_93', 'bn_93', 'mish_94', 'conv_94', 'bn_94', 'mish_95', 'shortcut_95', 'conv_96', 'bn_96', 'mish_97', 'conv_97', 'bn_97', 'mish_98', 'shortcut_98', 'conv_99', 'bn_99', 'mish_100', 'conv_100', 'bn_100', 'mish_101', 'shortcut_101', 'conv_102', 'bn_102', 'mish_103', 'concat_103', 'conv_104', 'bn_104', 'mish_105', 'conv_105', 'bn_105', 'leaky_106', 'conv_106', 'bn_106', 'leaky_107', 'conv_107', 'bn_107', 'leaky_108', 'pool_108', 'identity_109', 'pool_110', 'identity_111', 'pool_112', 'concat_113', 'conv_114', 'bn_114', 'leaky_115', 'conv_115', 'bn_115', 'leaky_116', 'conv_116', 'bn_116', 'leaky_117', 'conv_117', 'bn_117', 'leaky_118', 'upsample_118', 'identity_119', 'conv_120', 'bn_120', 'leaky_121', 'concat_121', 'conv_122', 'bn_122', 'leaky_123', 'conv_123', 'bn_123', 'leaky_124', 'conv_124', 'bn_124', 'leaky_125', 'conv_125', 'bn_125', 'leaky_126', 'conv_126', 'bn_126', 'leaky_127', 'conv_127', 'bn_127', 'leaky_128', 'upsample_128', 'identity_129', 'conv_130', 'bn_130', 'leaky_131', 'concat_131', 'conv_132', 'bn_132', 'leaky_133', 'conv_133', 'bn_133', 'leaky_134', 'conv_134', 'bn_134', 'leaky_135', 'conv_135', 'bn_135', 'leaky_136', 'conv_136', 'bn_136', 'leaky_137', 'conv_137', 'bn_137', 'leaky_138', 'conv_138', 'permute_139', 'yolo_139', 'identity_140', 'conv_141', 'bn_141', 'leaky_142', 'concat_142', 'conv_143', 'bn_143', 'leaky_144', 'conv_144', 'bn_144', 'leaky_145', 'conv_145', 'bn_145', 'leaky_146', 'conv_146', 'bn_146', 'leaky_147', 'conv_147', 'bn_147', 'leaky_148', 'conv_148', 'bn_148', 'leaky_149', 'conv_149', 'permute_150', 'yolo_150', 'identity_151', 'conv_152', 'bn_152', 'leaky_153', 'concat_153', 'conv_154', 'bn_154', 'leaky_155', 'conv_155', 'bn_155', 'leaky_156', 'conv_156', 'bn_156', 'leaky_157', 'conv_157', 'bn_157', 'leaky_158', 'conv_158', 'bn_158', 'leaky_159', 'conv_159', 'bn_159', 'leaky_160', 'conv_160', 'permute_161', 'yolo_161')\n",
      "379\n",
      "---------\n",
      "[327 353 379]\n",
      "['yolo_139', 'yolo_150', 'yolo_161']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m blob \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mdnn\u001b[39m.\u001b[39mblobFromImage(image, \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m \u001b[39m255.0\u001b[39m, (\u001b[39m416\u001b[39m, \u001b[39m416\u001b[39m),\n\u001b[0;32m     37\u001b[0m \tswapRB\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, crop\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     38\u001b[0m net\u001b[39m.\u001b[39msetInput(blob)\n\u001b[1;32m---> 39\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     40\u001b[0m layerOutputs \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39mforward(ln)\n\u001b[0;32m     41\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "# load the COCO class labels our YOLO model was trained on\n",
    "labelsPath = 'coco.names'\n",
    "LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "print(LABELS)\n",
    "# initialize a list of colors to represent each possible class label\n",
    "np.random.seed(42)\n",
    "COLORS = np.random.randint(0, 255, size=(len(LABELS), 3),\n",
    "\tdtype=\"uint8\")\n",
    "\n",
    "\n",
    "\n",
    "# derive the paths to the YOLO weights and model configuration\n",
    "weightsPath = \"yolov4-custom_2000.weights\"\n",
    "configPath = 'yolov4-custom.cfg'\n",
    "# load our YOLO object detector trained on COCO dataset (80 classes)\n",
    "print(\"[INFO] loading YOLO from disk...\")\n",
    "net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)\n",
    "print('done')\n",
    "\n",
    "\n",
    "# load our input image and grab its spatial dimensions\n",
    "img_path = 'fruit_images/all_3.jpg'\n",
    "image = cv2.imread(img_path)\n",
    "(H, W) = image.shape[:2]\n",
    "# determine only the *output* layer names that we need from YOLO\n",
    "ln = net.getLayerNames()\n",
    "print(ln)\n",
    "print(len(ln))\n",
    "print('---------')\n",
    "print(net.getUnconnectedOutLayers())\n",
    "ln = [ln[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "# construct a blob from the input image and then perform a forward\n",
    "# pass of the YOLO object detector, giving us our bounding boxes and\n",
    "# associated probabilities\n",
    "blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416),\n",
    "\tswapRB=True, crop=False)\n",
    "net.setInput(blob)\n",
    "start = time.time()\n",
    "layerOutputs = net.forward(ln)\n",
    "end = time.time()\n",
    "# show timing information on YOLO\n",
    "print(\"[INFO] YOLO took {:.6f} seconds\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39m# determine only the *output* layer names that we need from YOLO\u001b[39;00m\n\u001b[0;32m      6\u001b[0m ln \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39mgetLayerNames()\n\u001b[1;32m----> 7\u001b[0m ln \u001b[39m=\u001b[39m [ln[i[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m net\u001b[39m.\u001b[39mgetUnconnectedOutLayers()]\n\u001b[0;32m      8\u001b[0m \u001b[39m# construct a blob from the input image and then perform a forward\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m# pass of the YOLO object detector, giving us our bounding boxes and\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m# associated probabilities\u001b[39;00m\n\u001b[0;32m     11\u001b[0m blob \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mdnn\u001b[39m.\u001b[39mblobFromImage(image, \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m \u001b[39m255.0\u001b[39m, (\u001b[39m416\u001b[39m, \u001b[39m416\u001b[39m),\n\u001b[0;32m     12\u001b[0m \tswapRB\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, crop\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn [7], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39m# determine only the *output* layer names that we need from YOLO\u001b[39;00m\n\u001b[0;32m      6\u001b[0m ln \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39mgetLayerNames()\n\u001b[1;32m----> 7\u001b[0m ln \u001b[39m=\u001b[39m [ln[i[\u001b[39m0\u001b[39;49m] \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m net\u001b[39m.\u001b[39mgetUnconnectedOutLayers()]\n\u001b[0;32m      8\u001b[0m \u001b[39m# construct a blob from the input image and then perform a forward\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m# pass of the YOLO object detector, giving us our bounding boxes and\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m# associated probabilities\u001b[39;00m\n\u001b[0;32m     11\u001b[0m blob \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mdnn\u001b[39m.\u001b[39mblobFromImage(image, \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m \u001b[39m255.0\u001b[39m, (\u001b[39m416\u001b[39m, \u001b[39m416\u001b[39m),\n\u001b[0;32m     12\u001b[0m \tswapRB\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, crop\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "553532ff9cfa78d28a403dbc92c9038df31042aa047112c16f748ade1e5cdb7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
